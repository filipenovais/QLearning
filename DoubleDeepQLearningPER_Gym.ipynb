{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__\n",
    "# 0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size, alpha=0.6, beta=0.4):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.buffer_size = buffer_size\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.priorities = np.zeros((buffer_size,), dtype=np.float32)\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        max_prio = self.priorities.max() if self.buffer else 1.0\n",
    "\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append((state, action, reward, state_, done))\n",
    "        else:\n",
    "            self.buffer[self.position] = (state, action, reward, state_, done)\n",
    "        \n",
    "        self.priorities[self.position] = max_prio\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.buffer_size:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.position]\n",
    "        \n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "        \n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        batch = list(zip(*samples))\n",
    "        states = torch.tensor(np.array(batch[0]), dtype=torch.float32)\n",
    "        actions = torch.tensor(np.array(batch[1]), dtype=torch.int64)\n",
    "        rewards = torch.tensor(np.array(batch[2]), dtype=torch.float32)\n",
    "        states_ = torch.tensor(np.array(batch[3]), dtype=torch.float32)\n",
    "        dones = torch.tensor(np.array(batch[4]), dtype=torch.int64)\n",
    "        return states, actions, rewards, states_, dones, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, pi in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = pi\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network and Agent Classes\n",
    "\n",
    "# Define a neural network class that inherits from the PyTorch nn.Module class.\n",
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "\n",
    "        # Define the neural network layers and activation functions.\n",
    "        self.fc1 = nn.Linear(input_dims[0], 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "\n",
    "        # Define the optimizer and loss function for training the neural network.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # Define the forward pass of the neural network.\n",
    "    def forward(self, state):\n",
    "        layer1 = torch.relu(self.fc1(state))\n",
    "        layer2 = torch.relu(self.fc2(layer1))\n",
    "        actions = self.fc3(layer2)\n",
    "\n",
    "        return actions\n",
    "\n",
    "# Define an agent class for training the neural network.\n",
    "class Agent():\n",
    "    def __init__(self, input_dims, n_actions, buffer_size=10000, lr=2e-4, gamma=0.95,\n",
    "                epsilon=1.0, eps_dec=5e-5, eps_min=0.01):\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [a for a in range(self.n_actions)]\n",
    "\n",
    "        # Create instances of the neural networks for the agent.\n",
    "        self.Q_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        self.Target_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "\n",
    "        # Create Priority Replay Buffer\n",
    "        self.priority_replay_buffer = PrioritizedReplayBuffer(buffer_size)\n",
    "\n",
    "    # Update the target network initially to match the online network.\n",
    "    def update_target_network(self):\n",
    "        self.Target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "\n",
    "    # Define a function for choosing an action given an observation.\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # Use the neural network to predict the Q-values for the current state.\n",
    "            state = torch.tensor(state, dtype=torch.float).to(self.Q_network.device)\n",
    "            actions = self.Q_network.forward(state)\n",
    "            # Choose the action with the highest Q-value.\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            # Choose a random action with probability epsilon.\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Define a function for decrementing epsilon over time to decrease exploration.\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    # Define a function for training the neural network with a batch of experiences.\n",
    "    def learn(self, states, actions, rewards, states_, dones, indices, weights):\n",
    "        self.Q_network.optimizer.zero_grad()\n",
    "        # Convert the data to PyTorch tensors and move to the device for training.\n",
    "        states = states.to(self.Q_network.device)\n",
    "        actions = actions.to(self.Q_network.device)\n",
    "        rewards = rewards.to(self.Q_network.device)\n",
    "        states_ = states_.to(self.Target_network.device)\n",
    "        dones = dones.to(self.Target_network.device)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32).to(self.Q_network.device)\n",
    "\n",
    "        # Use the online network to predict the Q-values for the current states and select actions.\n",
    "        q_pred = self.Q_network.forward(states).gather(1, actions.reshape(-1, 1))\n",
    "\n",
    "        # Use the target network to predict the Q-values for the next states.\n",
    "        q_next, _ = torch.max(self.Target_network.forward(states_), dim=1)\n",
    "\n",
    "        # Calculate the target Q-values based on the current rewards and expected future rewards.\n",
    "        q_target = rewards + self.gamma * (1-dones) * q_next\n",
    "        q_target = q_target.detach()\n",
    "\n",
    "        # Calculate the mean squared error loss between the predicted and target Q-values.\n",
    "        loss  = (q_pred.squeeze() - q_target).pow(2) * weights\n",
    "        pi = loss + 0.01\n",
    "        loss = loss.mean()\n",
    "\n",
    "        # Perform backpropagation to update the online network weights.\n",
    "        loss.backward()\n",
    "        self.Q_network.optimizer.step()\n",
    "\n",
    "        self.priority_replay_buffer.update_priorities(indices, pi.data.cpu().numpy())\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.Q_network.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.Q_network.load_state_dict(torch.load(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score and epsilon curves\n",
    "def plot_score_epsilon(x_axis, scores, epsilons):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the first curve using the first y-axis.\n",
    "    ax1.plot(x_axis, scores, 'b-', label='score')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('score', color='b')\n",
    "\n",
    "    # Create a second y-axis object and plot the second curve using it.\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_axis, epsilons, 'r-', label='epsilon')\n",
    "    ax2.set_ylabel('epsilon', color='r')\n",
    "\n",
    "    # Add a legend to the plot.\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='lower left')\n",
    "\n",
    "    # Show the plot.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state dim (8,)\n",
      "action dim 4\n",
      "episode 10, last score -150.3, avg score nan epsilon 0.97\n",
      "episode 20, last score -177.3, avg score -150.3 epsilon 0.94\n",
      "episode 30, last score -233.4, avg score -163.8 epsilon 0.91\n",
      "episode 40, last score -118.3, avg score -187.0 epsilon 0.88\n",
      "episode 50, last score -78.8, avg score -169.8 epsilon 0.85\n",
      "episode 60, last score -83.9, avg score -151.6 epsilon 0.82\n",
      "episode 70, last score -153.8, avg score -140.3 epsilon 0.79\n",
      "episode 80, last score -112.4, avg score -142.2 epsilon 0.76\n",
      "episode 90, last score -83.6, avg score -138.5 epsilon 0.73\n",
      "episode 100, last score -94.8, avg score -132.4 epsilon 0.70\n",
      "episode 110, last score -53.6, avg score -128.7 epsilon 0.67\n",
      "episode 120, last score -28.9, avg score -119.0 epsilon 0.64\n",
      "episode 130, last score -84.6, avg score -104.1 epsilon 0.61\n",
      "episode 140, last score -155.4, avg score -89.3 epsilon 0.58\n",
      "episode 150, last score -45.1, avg score -93.0 epsilon 0.55\n",
      "episode 160, last score -89.8, avg score -89.6 epsilon 0.52\n",
      "episode 170, last score -114.7, avg score -90.2 epsilon 0.49\n",
      "episode 180, last score -294.7, avg score -86.3 epsilon 0.46\n",
      "episode 190, last score -29.3, avg score -104.5 epsilon 0.43\n",
      "episode 200, last score -5.9, avg score -99.1 epsilon 0.40\n",
      "episode 210, last score -164.3, avg score -90.2 epsilon 0.37\n",
      "episode 220, last score -78.2, avg score -101.3 epsilon 0.34\n",
      "episode 230, last score -72.9, avg score -106.2 epsilon 0.31\n",
      "episode 240, last score 42.9, avg score -105.0 epsilon 0.28\n",
      "episode 250, last score -100.3, avg score -85.2 epsilon 0.25\n",
      "episode 260, last score -32.2, avg score -90.7 epsilon 0.22\n",
      "episode 270, last score -32.1, avg score -85.0 epsilon 0.19\n",
      "episode 280, last score 17.4, avg score -76.7 epsilon 0.16\n",
      "episode 290, last score 114.2, avg score -45.5 epsilon 0.13\n",
      "episode 300, last score 29.4, avg score -31.1 epsilon 0.10\n",
      "episode 310, last score -45.8, avg score -27.6 epsilon 0.07\n",
      "episode 320, last score -714.4, avg score -15.7 epsilon 0.04\n",
      "episode 330, last score -54.4, avg score -79.4 epsilon 0.01\n",
      "episode 340, last score 40.0, avg score -77.5 epsilon 0.01\n",
      "episode 350, last score 148.4, avg score -77.8 epsilon 0.01\n",
      "episode 360, last score -10.8, avg score -52.9 epsilon 0.01\n",
      "episode 370, last score -5.3, avg score -50.8 epsilon 0.01\n",
      "episode 380, last score 1.0, avg score -48.1 epsilon 0.01\n",
      "episode 390, last score -27.3, avg score -49.8 epsilon 0.01\n",
      "episode 400, last score 93.7, avg score -63.9 epsilon 0.01\n",
      "episode 410, last score 81.3, avg score -57.5 epsilon 0.01\n",
      "episode 420, last score 49.0, avg score -44.8 epsilon 0.01\n",
      "episode 430, last score 129.4, avg score 31.5 epsilon 0.01\n",
      "episode 440, last score -111.0, avg score 49.9 epsilon 0.01\n",
      "episode 450, last score 107.6, avg score 34.8 epsilon 0.01\n",
      "episode 460, last score 1.7, avg score 30.7 epsilon 0.01\n",
      "episode 470, last score -592.8, avg score 32.0 epsilon 0.01\n",
      "episode 480, last score -296.9, avg score -26.7 epsilon 0.01\n",
      "episode 490, last score -354.3, avg score -56.5 epsilon 0.01\n",
      "episode 500, last score -25.0, avg score -89.2 epsilon 0.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the environment\n",
    "env_name = 'LunarLander'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Get the dimension of the state and action spaces\n",
    "state_dim = env.reset()[0].shape\n",
    "action_dim = env.action_space.n\n",
    "print(f'state dim {state_dim}')\n",
    "print(f'action dim {action_dim}')\n",
    "\n",
    "# Define the path to save the trained model\n",
    "save_path = 'models/'+env_name\n",
    "\n",
    "# Define the number of episodes to run\n",
    "n_episodes = 500\n",
    "eps_decay = 3e-3\n",
    "\n",
    "# Frequency at which to save the scores\n",
    "save_score_freq = 10\n",
    "\n",
    "# Number of episodes after which the target network is updated\n",
    "target_update_freq = 2\n",
    "\n",
    "# Initialize the replay buffer and define batch size\n",
    "buffer_size = 10000\n",
    "batch_size = 32\n",
    "\n",
    "# Create an instance of the agent class\n",
    "agent = Agent(input_dims=state_dim, n_actions=action_dim, buffer_size=buffer_size, eps_dec=eps_decay)\n",
    "\n",
    "# Initialize tracking variables\n",
    "end_all, x_axis, scores, epsilons = False, [], [], []\n",
    "# Run the main training loop for the specified number of episodes\n",
    "for i in range(n_episodes):\n",
    "    # Reset the environment and initialize the score, done flag, and observation\n",
    "    score = 0\n",
    "    done = False\n",
    "    state = env.reset()[0]\n",
    "    steps = 0\n",
    "\n",
    "    # Run the episode until the environment returns done\n",
    "    while not done:\n",
    "        # Choose an action based on the current observation and agent policy\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # Take a step in the environment and update the score, done flag, and observation\n",
    "        state_, reward, done, info, _ = env.step(action)\n",
    "        if steps>250:\n",
    "            #end_all = True\n",
    "            reward = -150\n",
    "            done = True\n",
    "            agent.priority_replay_buffer.add(state, action, reward, state_, done)  # Add the experience to the replay buffer\n",
    "            break\n",
    "\n",
    "        agent.priority_replay_buffer.add(state, action, reward, state_, done)  # Add the experience to the replay buffer\n",
    "\n",
    "        # Perform learning if enough experiences are stored in the replay buffer\n",
    "        if len(agent.priority_replay_buffer) > batch_size:\n",
    "            states, actions, rewards, states_, dones, indices, weights = agent.priority_replay_buffer.sample(batch_size)  # Sample a batch of experiences\n",
    "            agent.learn(states, actions, rewards, states_, dones, indices, weights)  # Train the agent with the sampled experiences\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "\n",
    "    if i % target_update_freq == 0:\n",
    "        agent.update_target_network()  # Update the target network weights periodically\n",
    "\n",
    "    # Print the episode statistics and track the score and epsilon value over time\n",
    "    if (i + 1) % save_score_freq == 0 or end_all:\n",
    "        avg_score = np.mean(scores[-save_score_freq:])\n",
    "        print(f'episode {i+1}, last score {score:.1f}, avg score {avg_score:.1f} epsilon {agent.epsilon:.2f}')\n",
    "        x_axis.append(i+1)\n",
    "        scores.append(score)\n",
    "        epsilons.append(agent.epsilon)\n",
    "        if end_all:\n",
    "            break\n",
    "\n",
    "    # Decrease the epsilon value to decrease exploration over time.\n",
    "    agent.decrement_epsilon()\n",
    "\n",
    "# Save the trained model\n",
    "agent.save(save_path)  \n",
    "\n",
    "# Plot the scores and epsilon values over time\n",
    "plot_score_epsilon(x_axis, scores, epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "State: [-0.00670242  1.4175996  -0.6788991   0.29684272  0.00777323  0.15378077\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.013311    1.4237086  -0.6661419   0.27147397  0.012997    0.10448559\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.01984949  1.4292195  -0.65733135  0.24490005  0.01644735  0.0690133\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.02629728  1.4341416  -0.64595944  0.21875529  0.01761208  0.02329676\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.03264704  1.4384661  -0.6336639   0.19221163  0.01631049 -0.02603408\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.03892183  1.442204   -0.62425804  0.16616407  0.01312113 -0.06379329\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.04513178  1.4453506  -0.61611784  0.13988404  0.00830077 -0.09641629\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.05126924  1.4479072  -0.60702217  0.11364373  0.00165928 -0.13284205\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.05733204  1.4498557  -0.5976652   0.08659    -0.00685236 -0.17024872\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.06333494  1.4511975  -0.5901698   0.05955981 -0.01686041 -0.20017977\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.06925764  1.4519408  -0.58012414  0.03286022 -0.02887455 -0.24030466\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.07508554  1.452077   -0.56824034  0.00570913 -0.04326553 -0.28784615\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.08084307  1.4516175  -0.5594455  -0.02097295 -0.05940612 -0.3228413\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.08653174  1.4505551  -0.5508393  -0.04803551 -0.07726489 -0.3572081\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.0921547   1.448879   -0.5426381  -0.0756264  -0.09677533 -0.3902446\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.09768982  1.4465866  -0.53165853 -0.10345013 -0.11850283 -0.43458968\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.1032237   1.4437002  -0.53160024 -0.13015822 -0.14022575 -0.434498\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.10875635  1.4402201  -0.53154457 -0.15686756 -0.16194513 -0.43442687\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.11436272  1.4361748  -0.5409566  -0.18205844 -0.18169577 -0.39504814\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.11996803  1.4315344  -0.5409098  -0.20876335 -0.20144376 -0.39499548\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.12557192  1.4262989  -0.5408625  -0.2354689  -0.22118789 -0.39491835\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.13123731  1.4204767  -0.5486449  -0.26154962 -0.23934674 -0.36320987\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.13690147  1.4140586  -0.5486051  -0.28825188 -0.25750318 -0.36316147\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.14262494  1.4070661  -0.5561873  -0.3137031  -0.2740591  -0.3311472\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.1484333   1.3995018  -0.56691587 -0.3388603  -0.28837964 -0.28643632\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.1542367   1.3913269  -0.5668572  -0.36605623 -0.30229673 -0.2783566\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.16012831  1.3825877  -0.5774648  -0.39088354 -0.3144135  -0.24233523\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.16601953  1.3732504  -0.5774704  -0.417562   -0.32653016 -0.24233277\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.1719676   1.3633369  -0.5846737  -0.4429156  -0.33712113 -0.21181948\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.17791548  1.3528252  -0.5846783  -0.4695912  -0.347712   -0.21181779\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.18392448  1.3417412  -0.5923959  -0.49469826 -0.35664898 -0.17873874\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.18965816  1.3310646  -0.5649971  -0.47662494 -0.36547673 -0.17655466\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.19530812  1.3206003  -0.5561417  -0.4673633  -0.37483707 -0.18720734\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20081559  1.3107746  -0.5411979  -0.43923557 -0.38498187 -0.20289679\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.20639782  1.3004043  -0.5507691  -0.46293068 -0.39292815 -0.15892565\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21180162  1.2905834  -0.53240335 -0.43871406 -0.40146852 -0.17080705\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2170227   1.2807175  -0.51425946 -0.4407366  -0.40989923 -0.16861463\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22186264  1.2708901  -0.4769488  -0.43883273 -0.4174985  -0.1519851\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22622529  1.2617248  -0.42964488 -0.40932062 -0.42465734 -0.14317718\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.23064356  1.2519952  -0.43669987 -0.4339881  -0.43021762 -0.11120567\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23500839  1.2422713  -0.4308555  -0.43390352 -0.4363317  -0.12228148\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23902297  1.2327945  -0.3961851  -0.42284626 -0.44208133 -0.11499359\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.24276142  1.2241304  -0.36793643 -0.3869511  -0.4485393  -0.12915882\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.24657884  1.2149119  -0.3778896  -0.4109493  -0.4527643  -0.08450007\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2500771   1.2065686  -0.34538314 -0.37226352 -0.45765084 -0.09773103\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.2536583   1.1976858  -0.35591608 -0.39551404 -0.46007493 -0.04848174\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.25679523  1.1890489  -0.31211278 -0.38439524 -0.46181577 -0.03481696\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.25961986  1.181071   -0.2804889  -0.35522774 -0.46399295 -0.04354357\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.26249766  1.1725309  -0.28722623 -0.37974745 -0.46460068 -0.0121546\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.26512653  1.163856   -0.26268286 -0.38561496 -0.4648221  -0.00442877\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.26761055  1.1551315  -0.24805903 -0.3878771  -0.46520522 -0.00766189\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.27015322  1.1458377  -0.2553931  -0.4126766  -0.4639597   0.02491027\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27249584  1.1367755  -0.23510091 -0.40248376 -0.4630337   0.01851984\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2745038   1.1277566  -0.20208792 -0.40041685 -0.4616087   0.02850007\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27609235  1.1195449  -0.15996812 -0.3645949  -0.46037897  0.02459462\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.27776298  1.1107992  -0.1704537  -0.3875819  -0.45665032  0.07457294\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.279492    1.1015074  -0.17796877 -0.41131884 -0.45108256  0.11135538\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28107777  1.092655   -0.16292365 -0.39204076 -0.44631478  0.09535551\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28240663  1.0844873  -0.1366643  -0.36181185 -0.44218433  0.08260908\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.2837921   1.0757642  -0.14390074 -0.38601488 -0.4363457   0.11677303\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.2852409   1.066479   -0.15190187 -0.41051623 -0.42870933  0.15272729\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28633276  1.0575436  -0.11649968 -0.39491835 -0.42078924  0.15840173\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28722626  1.0488052  -0.09653274 -0.3862553  -0.4130349   0.15508737\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.28821015  1.0395162  -0.10794093 -0.41008866 -0.40275866  0.20552447\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28891975  1.0311465  -0.07993081 -0.36946765 -0.39315087  0.19215615\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.28970402  1.0222241  -0.08943553 -0.39355692 -0.38141173  0.2347825\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.29042178  1.013653   -0.08207236 -0.3782201  -0.3704783   0.21866861\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2910304   1.0053141  -0.07076876 -0.3680773  -0.3600031   0.2095029\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2913595   0.9971601  -0.04320237 -0.35985777 -0.34917068  0.21664926\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.29177147  0.9884601  -0.05378283 -0.38366333 -0.33597714  0.26387116\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.29191622  0.9798426  -0.02757213 -0.38001415 -0.32231057  0.27333167\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2916545   0.97207177  0.01265692 -0.34241995 -0.30827397  0.2807322\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.29148403  0.9637372   0.00111973 -0.36714122 -0.291813    0.32921985\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.29121763  0.9557163   0.01101595 -0.35344097 -0.2757371   0.3215176\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.29077974  0.94817376  0.02822181 -0.33237135 -0.25980368  0.31866843\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.29012913  0.9410166   0.04926733 -0.31537905 -0.24369974  0.3220791\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.28947797  0.9332628   0.04925913 -0.3420669  -0.22759607  0.32207328\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2886927   0.92620915  0.06290587 -0.31116447 -0.21179244  0.31607285\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.28790697  0.9185588   0.06289895 -0.33785167 -0.19598906  0.31606737\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28685123  0.91170686  0.08946685 -0.30248106 -0.17980061  0.32376915\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.28579512  0.9042584   0.08946064 -0.32916945 -0.16361246  0.3237633\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28476328  0.896916    0.08744661 -0.3246723  -0.14788991  0.3144505\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2835819   0.890179    0.10222507 -0.29792166 -0.132031    0.31717855\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.28240037  0.88284534  0.10222058 -0.3246093  -0.11617235  0.31717306\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.28105244  0.87564385  0.11831603 -0.31886014 -0.09981182  0.32721066\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27958903  0.8685701   0.12952642 -0.31335142 -0.08314365  0.3333634\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27805692  0.86164945  0.13625292 -0.30671787 -0.06635165  0.33583996\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27643424  0.8556454   0.14517218 -0.266174   -0.04945211  0.337991\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.2748115   0.84904504  0.14517006 -0.29286468 -0.0325529   0.33798435\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27333578  0.84273577  0.13123326 -0.2801298  -0.01642243  0.32260954\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27199656  0.8365315   0.1182605  -0.27564642 -0.00095816  0.30928522\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.27048737  0.8308574   0.13445398 -0.2522439   0.01528789  0.32492125\n",
      "  0.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.26897827  0.82458705  0.1344542  -0.27893284  0.03153365  0.32491535\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.26766127  0.81861377  0.11606421 -0.26587272  0.04697926  0.3089121\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2662344   0.8128201   0.12643959 -0.25808355  0.06304506  0.32131636\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.26477942  0.8077509   0.12886393 -0.22607063  0.07949722  0.3290431\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.26341572  0.8030634   0.11994343 -0.20928337  0.0957692   0.3254399\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.26196045  0.79780227  0.13149005 -0.23479101  0.10969698  0.27855587\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.26042467  0.7919585   0.14162555 -0.26065016  0.12157347  0.23752971\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.25881615  0.7855322   0.15078399 -0.28646585  0.13159014  0.20033379\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.25712046  0.77851886  0.16173448 -0.31241986  0.13939744  0.15614559\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.25536507  0.77092665  0.16925159 -0.33803377  0.1456585   0.12522098\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.2535273   0.7627566   0.17963108 -0.3635235   0.1497983   0.08279635\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.25178424  0.75396425  0.16773859 -0.3914502   0.1563641   0.13131545\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2502572   0.74545544  0.14669205 -0.37881523  0.16236722  0.12006296\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.24881181  0.7372623   0.13845494 -0.3648105   0.1684547   0.12174962\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.24727826  0.7284803   0.14949925 -0.39075682  0.17231625  0.07723083\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2458767   0.7197184   0.13657758 -0.38983807  0.17590477  0.07176994\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.24451628  0.7111425   0.13222793 -0.3816125   0.17972997  0.07650398\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.24308643  0.70199174  0.14098825 -0.40694055  0.18173856  0.0401718\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.24191742  0.69328636  0.11549876 -0.3870756   0.1831436   0.0281009\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.24066448  0.6839926   0.12600704 -0.41296357  0.18242605 -0.01435095\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.2394722   0.67408776  0.11840294 -0.44032076  0.18325326  0.01654434\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23827013  0.6646095   0.11876379 -0.4214416   0.18470164  0.02896794\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23716827  0.6557624   0.10850243 -0.39341888  0.18639185  0.03380417\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23606214  0.64687085  0.10859378 -0.39543095  0.18841887  0.04054049\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2352087   0.6386779   0.08372376 -0.3643358   0.19004866  0.0325957\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.23427744  0.62990713  0.09350117 -0.3897686   0.18966763 -0.00762078\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.23327132  0.6205659   0.10295562 -0.4148653   0.18731526 -0.04704782\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23245545  0.6113481   0.0843753  -0.40932992  0.1845141  -0.05602287\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23179388  0.6028555   0.06889885 -0.37710613  0.18176155 -0.05505117\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.2312068   0.5937399   0.05952572 -0.40503863  0.18094161 -0.01639882\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23064928  0.5847549   0.05629977 -0.39926955  0.18039258 -0.01098076\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.23008093  0.5759016   0.05693107 -0.39346668  0.18029758 -0.00190003\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22953352  0.5674725   0.05436534 -0.37467453  0.18067339  0.00751616\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22905488  0.55919874  0.04738742 -0.36778244  0.1811552   0.00963615\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.22848101  0.55034727  0.05933892 -0.3931628   0.17919527 -0.03919857\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22797485  0.5420729   0.05223071 -0.36755463  0.17757423 -0.03242081\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.22739176  0.5332307   0.06194355 -0.3925549   0.1739273  -0.07293854\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2269113   0.52525896  0.05136751 -0.35391957  0.17059767 -0.06659253\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.22636628  0.51670164  0.05948079 -0.37976107  0.16561396 -0.09967411\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.22574148  0.50755423  0.06947531 -0.4057885   0.15861647 -0.13994995\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.22517553  0.4977926   0.06209068 -0.43327436  0.15312812 -0.10976702\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22455736  0.48851287  0.06656788 -0.41195354  0.14838701 -0.09482209\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22396526  0.48002768  0.06344433 -0.37670553  0.14416231 -0.084494\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.2233036   0.47095957  0.07218679 -0.40245813  0.13815536 -0.12013879\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.22272773  0.46126285  0.06138647 -0.43061474  0.13436832 -0.07574087\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.22209898  0.45241737  0.06583829 -0.39287254  0.13142319 -0.05890253\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2215127   0.44432595  0.06123037 -0.35938835  0.12883028 -0.05185808\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.22083426  0.43566027  0.07283221 -0.38471982  0.12386836 -0.09923846\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.22009262  0.42640215  0.08075619 -0.41094223  0.11731105 -0.13114622\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.21941638  0.41652498  0.07253261 -0.4386077   0.11243351 -0.09755094\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2186472   0.40723228  0.0809861  -0.41270643  0.10839259 -0.08081846\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21779409  0.3981216   0.08872829 -0.40467563  0.10499811 -0.06788975\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21704283  0.38985115  0.07855157 -0.36733863  0.10160125 -0.06793728\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21652551  0.38217533  0.05586645 -0.340874    0.09750246 -0.08197588\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.21592112  0.37390473  0.06677686 -0.36717975  0.09121592 -0.12573084\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.21521993  0.3650473   0.07893668 -0.3931509   0.08248045 -0.17470942\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.21459022  0.35557288  0.0699545  -0.42071798  0.07556573 -0.13829437\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2139915   0.34621146  0.06683513 -0.41572458  0.06867427 -0.1378291\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2133604   0.33711085  0.06974137 -0.40418446  0.06211437 -0.13119796\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21288987  0.32794464  0.05433567 -0.40710527  0.05491634 -0.14396062\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21229735  0.3194313   0.06574689 -0.37814537  0.04849323 -0.12846226\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21162005  0.31147134  0.07366195 -0.35359266  0.0426344  -0.11717661\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.21098566  0.30420887  0.06939372 -0.32262227  0.03674789 -0.11773012\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.21026559  0.29635715  0.08015503 -0.34878665  0.02870082 -0.16094166\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.20962243  0.28790003  0.07050742 -0.3757659   0.02258838 -0.12224858\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20908947  0.28030872  0.05990114 -0.33730486  0.01606929 -0.13038193\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20870881  0.27266467  0.04535611 -0.3396764   0.00887363 -0.14391318\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20834808  0.26579913  0.04341542 -0.30510923  0.00162385 -0.14499566\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20807704  0.25954357  0.03485969 -0.27803376 -0.00603402 -0.15315732\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.20788126  0.25269693  0.02541791 -0.30432987 -0.01180025 -0.11532424\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20758776  0.24641012  0.03477747 -0.2794663  -0.01715652 -0.10712548\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2074047   0.24027486  0.02428738 -0.2727572  -0.02306274 -0.11812458\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20709181  0.23490372  0.03676449 -0.23881312 -0.02846536 -0.10805235\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.20668793  0.22893248  0.0481692  -0.26555437 -0.03615171 -0.15372732\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.20637426  0.22235492  0.03686085 -0.29247853 -0.04157847 -0.10853505\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20615391  0.21605717  0.0280762  -0.28008223 -0.0475486  -0.11940251\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2057186   0.21033326  0.04874073 -0.25456703 -0.05269791 -0.10298614\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.20536065  0.2040134   0.0390368  -0.28100032 -0.05590194 -0.0640806\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.2051116   0.19764191  0.02874668 -0.28332633 -0.05970285 -0.07601841\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20497008  0.19169134  0.01868727 -0.2646568  -0.06418708 -0.08968446\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20495424  0.18603924  0.00688168 -0.2514409  -0.06942829 -0.1048245\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20477681  0.18034966  0.02240587 -0.25309682 -0.07404462 -0.09232654\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20442972  0.1746611   0.03871978 -0.25302976 -0.07801574 -0.07942263\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20402722  0.1689387   0.04413902 -0.25453636 -0.08186734 -0.07703206\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20347242  0.16354638  0.05889254 -0.2398491  -0.08525167 -0.06768633\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.20284005  0.1575541   0.06861512 -0.2666374  -0.09057979 -0.10656218\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20223364  0.1521435   0.06646548 -0.24083787 -0.09635647 -0.11553383\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.20153332  0.14611052  0.07826894 -0.26868635 -0.10452896 -0.16344963\n",
      "  0.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.20092602  0.13948818  0.06659967 -0.2947492  -0.11035346 -0.1164902\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.20032945  0.13300668  0.06582226 -0.28853506 -0.11647855 -0.12250181\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.19977179  0.12652017  0.06232001 -0.28881538 -0.12299512 -0.13033149\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.19905844  0.12035523  0.07751326 -0.27452046 -0.1291458  -0.12301344\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.1981081   0.11427999  0.10039028 -0.27048475 -0.13448383 -0.10676068\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.19704476  0.10847291  0.1115279  -0.25857028 -0.13966265 -0.10357647\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.19592075  0.10277139  0.11761459 -0.25390106 -0.14486627 -0.10407233\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.194586    0.09771081  0.13828702 -0.22538684 -0.14965698 -0.09581436\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.19318084  0.09203404  0.14710814 -0.2529766  -0.15624642 -0.13178924\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.19153777  0.08658293  0.17021097 -0.24290475 -0.16214487 -0.11796856\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.18980512  0.08051914  0.18142173 -0.2704115  -0.17030995 -0.16330186\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.18799868  0.07384373  0.19066396 -0.29786462 -0.18034926 -0.20078616\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.18620129  0.06760705  0.19034393 -0.2785121  -0.19100381 -0.2130907\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.18431234  0.06074639  0.20181437 -0.3066403  -0.20401824 -0.26028806\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.18223152  0.05391291  0.22050722 -0.30547515 -0.21655485 -0.25073224\n",
      "  0.          0.        ]\n",
      "Action: 3\n",
      "---\n",
      "State: [-0.18006802  0.04645279  0.23087123 -0.33375594 -0.23124915 -0.29388633\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.17769393  0.0390657   0.25141025 -0.33057538 -0.24547121 -0.2844415\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.17515841  0.0317608   0.26729155 -0.32702276 -0.25948006 -0.28017673\n",
      "  0.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.17247124  0.02465666  0.28236467 -0.31821525 -0.27344406 -0.27927992\n",
      "  1.          0.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.16950664  0.01897803  0.295568   -0.25336355 -0.26911134  0.09153584\n",
      "  1.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.1664836   0.0134255   0.27752104 -0.24243076 -0.24383691  0.5054605\n",
      "  1.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.16342898  0.00801331  0.2603956  -0.23373765 -0.19815463  0.913717\n",
      "  1.          0.        ]\n",
      "Action: 1\n",
      "---\n",
      "State: [-0.16034965  0.00258491  0.24665646 -0.23426217 -0.13667057  1.2298294\n",
      "  1.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.15722437 -0.00312619  0.24343848 -0.24897645 -0.06793963  1.3747953\n",
      "  1.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.15409651 -0.00934577  0.24265082 -0.2747411   0.00146303  1.3882129\n",
      "  1.          0.        ]\n",
      "Action: 0\n",
      "---\n",
      "State: [-0.15098591 -0.01612844  0.24187967 -0.30295464  0.06983187  1.3674337\n",
      "  1.          1.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.14825173 -0.02175725  0.22220564 -0.25345415  0.11790988  0.9581245\n",
      "  0.          1.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.14577885 -0.02692619  0.2185061  -0.23225835  0.14662446  0.5743689\n",
      "  0.          1.        ]\n",
      "Action: 2\n",
      "---\n",
      "State: [-0.1435916  -0.03107288  0.2006342  -0.1861727   0.16474047  0.36236602\n",
      "  1.          1.        ]\n",
      "Action: 1\n",
      "STEPS:  209\n"
     ]
    }
   ],
   "source": [
    "#env_name = 'CartPole-v1'\n",
    "env_name = 'LunarLander'\n",
    "load_path = 'models/' + env_name\n",
    "\n",
    "# Create an instance of the environment with rendering enabled.\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "# Initialize and load the agent.\n",
    "agent = Agent(input_dims=env.reset()[0].shape, n_actions=env.action_space.n)\n",
    "agent.load(load_path)\n",
    "\n",
    "# Set the exploration rate to zero to force the agent to choose actions based on its learned policy.\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "# Reset the environment and obtain the initial state.\n",
    "state = env.reset()[0]\n",
    "\n",
    "# Run a single episode of the environment using the agent's learned policy.\n",
    "done = False\n",
    "n_episodes = 1\n",
    "steps = 0\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        # Print the current observation and chosen action.\n",
    "        print('---')\n",
    "        print('State:', state)\n",
    "        action = agent.choose_action(state)\n",
    "        print('Action:', action)\n",
    "\n",
    "        # Take a step in the environment and render the current state.\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        env.render()\n",
    "        steps += 1\n",
    "        #time.sleep(0.1)  # Optional delay for visualization purposes.\n",
    "\n",
    "    # Reset the environment and the done flag for the next episode.\n",
    "    done = False\n",
    "    env.reset(seed=42)\n",
    "    print('STEPS: ', steps)\n",
    "# Close the environment viewer.\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
