{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.__version__\n",
    "# 0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST gym environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#env = gym.make('LunarLander')\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "#env = gym.make('FrozenLake-v1')\n",
    "#env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "\n",
    "state = env.reset(seed=42)\n",
    "action_size = env.action_space.n\n",
    "print('Number of actions:', action_size)\n",
    "done = False\n",
    "n_episodes = 1\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        action = np.random.randint(0, action_size)\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        print('Action:', action)\n",
    "        print('State:', state)\n",
    "        print('Reward:', reward)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "    done = False\n",
    "    env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer Class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # Initialize the replay buffer with a given maximum size.\n",
    "        self.buffer_size = buffer_size\n",
    "        # Create an empty buffer list to store experiences.\n",
    "        self.buffer = []\n",
    "        # Initialize the starting position of the buffer.\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, state_, done):\n",
    "        # Add a new experience to the buffer.\n",
    "        experience = (state, action, reward, state_, done)\n",
    "        # Add a None element to the buffer list if it's not full yet.\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(None)\n",
    "        # Add the experience to the buffer at the current position.\n",
    "        self.buffer[self.position] = experience\n",
    "        # Increment the position and wrap around if it goes beyond the buffer size.\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Sample a random batch of experiences from the buffer.\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        # Unzip the batch of experiences into separate lists of states, actions, rewards, and next states.\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        # Convert to a PyTorch tensor with various data type.\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64).reshape(-1,1),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.int64),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the current size of the buffer.\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network and Agent Classes\n",
    "\n",
    "# Define a neural network class that inherits from the PyTorch nn.Module class.\n",
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "\n",
    "        # Define the neural network layers and activation functions.\n",
    "        self.fc1 = nn.Linear(input_dims[0], 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "\n",
    "        # Define the optimizer and loss function for training the neural network.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # Define the forward pass of the neural network.\n",
    "    def forward(self, state):\n",
    "        layer1 = torch.relu(self.fc1(state))\n",
    "        layer2 = torch.relu(self.fc2(layer1))\n",
    "        actions = self.fc3(layer2)\n",
    "\n",
    "        return actions\n",
    "\n",
    "# Define an agent class for training the neural network.\n",
    "class Agent():\n",
    "    def __init__(self, input_dims, n_actions, lr=2e-4, gamma=0.95,\n",
    "                epsilon=1.0, eps_dec=1e-4, eps_min=0.05):\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        # Create instances of the neural networks for the agent.\n",
    "        self.Q_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        self.Target_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "\n",
    "    # Update the target network initially to match the online network.\n",
    "    def update_target_network(self):\n",
    "        self.Target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "\n",
    "    # Define a function for choosing an action given an observation.\n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # Use the neural network to predict the Q-values for the current state.\n",
    "            state = torch.tensor(state, dtype=torch.float).to(self.Q_network.device)\n",
    "            actions = self.Q_network.forward(state)\n",
    "            # Choose the action with the highest Q-value.\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            # Choose a random action with probability epsilon.\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Define a function for decrementing epsilon over time to decrease exploration.\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    # Define a function for training the neural network with a batch of experiences.\n",
    "    def learn(self, states, actions, rewards, states_, dones):\n",
    "        self.Q_network.optimizer.zero_grad()\n",
    "        # Convert the data to PyTorch tensors and move to the device for training.\n",
    "        states = torch.tensor(states).to(self.Q_network.device)\n",
    "        actions = torch.tensor(actions).to(self.Q_network.device)\n",
    "        rewards = torch.tensor(rewards).to(self.Q_network.device)\n",
    "        states_ = torch.tensor(states_).to(self.Target_network.device)\n",
    "        dones = torch.tensor(dones).to(self.Target_network.device)\n",
    "\n",
    "        # Use the online network to predict the Q-values for the current states and select actions.\n",
    "        q_pred = self.Q_network.forward(states).gather(1, actions)\n",
    "        \n",
    "        # Use the target network to predict the Q-values for the next states.\n",
    "        q_next, _ = torch.max(self.Target_network.forward(states_), dim=1)\n",
    "\n",
    "        # Calculate the target Q-values based on the current rewards and expected future rewards.\n",
    "        q_target = rewards + self.gamma * (1 - dones) * q_next\n",
    "\n",
    "        # Calculate the mean squared error loss between the predicted and target Q-values.\n",
    "        loss = self.Q_network.loss(q_target, q_pred.squeeze()).to(self.Q_network.device)\n",
    "        # Perform backpropagation to update the online network weights.\n",
    "        loss.backward()\n",
    "        self.Q_network.optimizer.step()\n",
    "        # Decrease the epsilon value to decrease exploration over time.\n",
    "        self.decrement_epsilon()\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save(self.Q_network.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.Q_network.load_state_dict(torch.load(path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot score and epsilon curves\n",
    "def plot_score_epsilon(x_axis, scores, epsilons):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the first curve using the first y-axis.\n",
    "    ax1.plot(x_axis, scores, 'b-', label='score')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('score', color='b')\n",
    "\n",
    "    # Create a second y-axis object and plot the second curve using it.\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_axis, epsilons, 'r-', label='epsilon')\n",
    "    ax2.set_ylabel('epsilon', color='r')\n",
    "\n",
    "    # Add a legend to the plot.\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='lower left')\n",
    "\n",
    "    # Show the plot.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env_name = 'CartPole-v1'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "# Get the dimension of the state and action spaces\n",
    "state_dim = env.reset()[0].shape\n",
    "action_dim = env.action_space.n\n",
    "print(f'state dim {state_dim}')\n",
    "print(f'action dim {action_dim}')\n",
    "\n",
    "# Define the path to save the trained model\n",
    "save_path = 'models/'+env_name\n",
    "\n",
    "# Define the number of episodes to run\n",
    "n_episodes = 300\n",
    "# Frequency at which to save the scores\n",
    "save_score_freq = 5\n",
    "\n",
    "# Number of episodes after which the target network is updated\n",
    "target_update_freq = 5\n",
    "\n",
    "# Initialize the replay buffer and define batch size\n",
    "buffer_size = 1000\n",
    "batch_size = 64\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "# Create an instance of the agent class\n",
    "agent = Agent(input_dims=state_dim, n_actions=action_dim)\n",
    "\n",
    "# Initialize tracking variables\n",
    "x_axis, scores, epsilons = [], [], []\n",
    "# Run the main training loop for the specified number of episodes\n",
    "for i in range(n_episodes):\n",
    "    # Reset the environment and initialize the score, done flag, and observation\n",
    "    score = 0\n",
    "    done = False\n",
    "    state = env.reset()[0]\n",
    "    steps = 0\n",
    "\n",
    "    # Run the episode until the environment returns done\n",
    "    while not done:\n",
    "        # Choose an action based on the current observation and agent policy\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # Take a step in the environment and update the score, done flag, and observation\n",
    "        state_, reward, done, info, _ = env.step(action)\n",
    "        replay_buffer.add(state, action, reward, state_, done)  # Add the experience to the replay buffer\n",
    "\n",
    "        # Perform learning if enough experiences are stored in the replay buffer\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            states, actions, rewards, states_, dones = replay_buffer.sample(batch_size)  # Sample a batch of experiences\n",
    "            agent.learn(states, actions, rewards, states_, dones)  # Train the agent with the sampled experiences\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "    if i % target_update_freq == 0:\n",
    "        agent.update_target_network()  # Update the target network weights periodically\n",
    "\n",
    "    # Print the episode statistics and track the score and epsilon value over time\n",
    "    if (i + 1) % save_score_freq == 0:\n",
    "        avg_score = np.mean(scores[-save_score_freq:])\n",
    "        print(f'episode {i+1}, last score {score:.1f}, avg score {avg_score:.1f} epsilon {agent.epsilon:.2f}')\n",
    "        x_axis.append(i+1)\n",
    "        scores.append(score)\n",
    "        epsilons.append(agent.epsilon)\n",
    "\n",
    "# Save the trained model\n",
    "agent.save(save_path)  \n",
    "\n",
    "# Plot the scores and epsilon values over time\n",
    "plot_score_epsilon(x_axis, scores, epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "load_path = 'models/' + env_name\n",
    "\n",
    "# Create an instance of the environment with rendering enabled.\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "\n",
    "# Initialize and load the agent.\n",
    "agent = Agent(input_dims=env.reset()[0].shape, n_actions=env.action_space.n)\n",
    "agent.load(load_path)\n",
    "\n",
    "# Set the exploration rate to zero to force the agent to choose actions based on its learned policy.\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "# Reset the environment and obtain the initial state.\n",
    "state = env.reset()[0]\n",
    "\n",
    "# Run a single episode of the environment using the agent's learned policy.\n",
    "done = False\n",
    "n_episodes = 1\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        # Print the current observation and chosen action.\n",
    "        print('---')\n",
    "        print('State:', state)\n",
    "        action = agent.choose_action(state)\n",
    "        print('Action:', action)\n",
    "\n",
    "        # Take a step in the environment and render the current state.\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        env.render()\n",
    "        #time.sleep(0.1)  # Optional delay for visualization purposes.\n",
    "\n",
    "    # Reset the environment and the done flag for the next episode.\n",
    "    done = False\n",
    "    env.reset(seed=42)\n",
    "\n",
    "# Close the environment viewer.\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
