{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.26.2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__\n",
    "# 0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions: 4\n",
      "Action: 1\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "Action: 3\n",
      "Observation: 4\n",
      "Reward: 0.0\n",
      "Action: 0\n",
      "Observation: 8\n",
      "Reward: 0.0\n",
      "Action: 3\n",
      "Observation: 9\n",
      "Reward: 0.0\n",
      "Action: 2\n",
      "Observation: 5\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "# TEST gym environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#env = gym.make('LunarLander')\n",
    "#env = gym.make('CartPole-v0')\n",
    "#env = gym.make('FrozenLake-v1')\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "\n",
    "observation = env.reset(seed=42)\n",
    "action_size = env.action_space.n\n",
    "print('Number of actions:', action_size)\n",
    "done = False\n",
    "n_episodes = 1\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        action = np.random.randint(0, action_size)\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        print('Action:', action)\n",
    "        print('Observation:', observation)\n",
    "        print('Reward:', reward)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "    done = False\n",
    "    env.reset()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an action based on epsilon-greedy strategy\n",
    "def choose_action(observation, q_table, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        action = np.random.randint(0, 4)  # Explore: choose a random action\n",
    "    else:\n",
    "        action = np.argmax(q_table[observation])  # Exploit: choose the action with the highest Q-value\n",
    "\n",
    "    return action\n",
    "\n",
    "# Define a Q-learning training function for a single episode\n",
    "def q_learning_episode(env, q_table):\n",
    "    # Define hyperparameters\n",
    "    epsilon = 0.2  # Exploration probability\n",
    "    alpha = 0.1    # Learning rate\n",
    "    gamma = 0.9    # Discount factor\n",
    "\n",
    "    done = False   # Initialize the 'done' flag (episode termination)\n",
    "    steps = 0      # Initialize the step counter\n",
    "    observation = env.reset(seed=42)[0]  # Reset the environment and get the initial observation\n",
    "\n",
    "    # Run the episode until it's done\n",
    "    while not done:\n",
    "        # Choose action based on policy\n",
    "        action = choose_action(observation, q_table, epsilon)\n",
    "\n",
    "        # Perform the action and receive the new observation, reward, and done flag\n",
    "        new_observation, reward, done, _, _= env.step(action)\n",
    "\n",
    "        # Modify the reward based on the new observation\n",
    "        if new_observation in [5, 7, 11, 12]:\n",
    "            reward = -1\n",
    "        elif new_observation == 15:\n",
    "            reward = 10\n",
    "        else:\n",
    "            reward = -0.1\n",
    "\n",
    "        # Calculate the maximum Q-value for the new observation\n",
    "        new_observation_max = np.max(q_table[new_observation])\n",
    "\n",
    "        # Update the Q-value in the Q-table using the Q-learning formula\n",
    "        q_table[observation, action] += alpha * (reward + gamma * new_observation_max - q_table[observation, action])\n",
    "\n",
    "        # Set the new observation as the current observation\n",
    "        observation = new_observation\n",
    "\n",
    "        # Increment the step counter\n",
    "        steps += 1\n",
    "\n",
    "        # Terminate the episode if it reaches 5000 steps\n",
    "        if steps >= 5000:\n",
    "            break\n",
    "\n",
    "    return q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Table - SHAPE: (16, 4)\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilizador\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left</th>\n",
       "      <th>Down</th>\n",
       "      <th>Right</th>\n",
       "      <th>Up</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.522</td>\n",
       "      <td>5.480</td>\n",
       "      <td>-0.208</td>\n",
       "      <td>1.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.173</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.110</td>\n",
       "      <td>0.912</td>\n",
       "      <td>-0.101</td>\n",
       "      <td>-0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.344</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.218</td>\n",
       "      <td>6.211</td>\n",
       "      <td>-0.850</td>\n",
       "      <td>0.961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.344</td>\n",
       "      <td>3.138</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>-0.047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.187</td>\n",
       "      <td>-0.613</td>\n",
       "      <td>7.017</td>\n",
       "      <td>1.440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.506</td>\n",
       "      <td>7.909</td>\n",
       "      <td>2.614</td>\n",
       "      <td>-0.686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.141</td>\n",
       "      <td>7.569</td>\n",
       "      <td>-0.271</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.613</td>\n",
       "      <td>5.901</td>\n",
       "      <td>8.900</td>\n",
       "      <td>5.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.811</td>\n",
       "      <td>2.840</td>\n",
       "      <td>10.000</td>\n",
       "      <td>1.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Left   Down   Right     Up\n",
       "0   1.522  5.480  -0.208  1.521\n",
       "1  -0.173 -0.469   0.019 -0.165\n",
       "2  -0.110  0.912  -0.101 -0.105\n",
       "3  -0.095 -0.344  -0.095 -0.086\n",
       "4   3.218  6.211  -0.850  0.961\n",
       "5   0.000  0.000   0.000  0.000\n",
       "6  -0.344  3.138  -0.271 -0.047\n",
       "7   0.000  0.000   0.000  0.000\n",
       "8   3.187 -0.613   7.017  1.440\n",
       "9   3.506  7.909   2.614 -0.686\n",
       "10  0.141  7.569  -0.271  0.169\n",
       "11  0.000  0.000   0.000  0.000\n",
       "12  0.000  0.000   0.000  0.000\n",
       "13 -0.613  5.901   8.900  5.144\n",
       "14  5.811  2.840  10.000  1.598\n",
       "15  0.000  0.000   0.000  0.000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Define a function to train a Q-learning agent\n",
    "def train_agent():\n",
    "    \n",
    "    # Create a FrozenLake environment with no slipperiness\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "    \n",
    "    # Get the number of actions and states in the environment\n",
    "    number_of_actions = env.action_space.n\n",
    "    number_of_states = env.observation_space.n\n",
    "\n",
    "    # Initialize the Q-table with zeros\n",
    "    q_table = np.array(np.zeros((number_of_states, number_of_actions)))\n",
    "    \n",
    "    # Print information about the shape of the Q-table\n",
    "    print('Q Table - SHAPE:', q_table.shape)\n",
    "    print(\"----------------\")\n",
    "    \n",
    "    # Train the agent for a fixed number of episodes\n",
    "    n_episodes = 200\n",
    "    for e in range(n_episodes):\n",
    "        q_table = q_learning_episode(env, q_table)\n",
    "    \n",
    "    # Return the final Q-table\n",
    "    return q_table\n",
    "\n",
    "# Train the agent and get the final Q-table\n",
    "q_table = train_agent()\n",
    "\n",
    "# Create a Pandas DataFrame from the Q-table and write it to a CSV file\n",
    "df = pd.DataFrame(q_table, columns=['Left', 'Down', 'Right', 'Up']).round(3)\n",
    "df.to_csv('q_table.csv', index=False, float_format=\"%.3f\")\n",
    "\n",
    "# Print the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilizador\\AppData\\Roaming\\Python\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action:  1\n",
      "Observation:  4\n",
      "Number of steps:  1\n",
      "Action:  1\n",
      "Observation:  8\n",
      "Number of steps:  2\n",
      "Action:  2\n",
      "Observation:  9\n",
      "Number of steps:  3\n",
      "Action:  1\n",
      "Observation:  13\n",
      "Number of steps:  4\n",
      "Action:  2\n",
      "Observation:  14\n",
      "Number of steps:  5\n",
      "Action:  2\n",
      "Observation:  15\n",
      "Number of steps:  6\n"
     ]
    }
   ],
   "source": [
    "# Define a function to evaluate the performance of a Q-learning agent\n",
    "def evaluate_q_learning(env, q_table):\n",
    "    \n",
    "    # Define a policy function that selects actions based on Q-values\n",
    "    def policy(q_table, observation):\n",
    "        action = np.argmax(q_table[observation])\n",
    "        return action\n",
    "    \n",
    "    # Reset the environment and set the initial observation\n",
    "    observation = env.reset(seed=42)[0]\n",
    "    \n",
    "    # Initialize variables for tracking the number of steps and whether the episode is done\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    # Run the policy until the episode is complete\n",
    "    while not done:\n",
    "        \n",
    "        # Select an action based on the policy\n",
    "        action = policy(q_table, observation)\n",
    "        \n",
    "        # Take a step in the environment and get the resulting observation, reward, and done flag\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        # Render the environment\n",
    "        env.render()\n",
    "        \n",
    "        # Increment the step counter and print information about the current step\n",
    "        steps += 1\n",
    "        print('Action: ', action)\n",
    "        print('Observation: ', observation)\n",
    "        print('Number of steps: ', steps)\n",
    "    \n",
    "    # Close the environment\n",
    "    env.close()\n",
    "\n",
    "# Create an instance of the FrozenLake environment with rendering enabled\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\", is_slippery=False)\n",
    "\n",
    "# Set the initial observation by resetting the environment\n",
    "observation = env.reset(seed=42)\n",
    "\n",
    "# Call the evaluate_q_learning function with the environment and a Q-table as arguments\n",
    "evaluate_q_learning(env, q_table)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sistemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d0072dcc2df830b042d5d7447fe5080206d55ec0c42aabe900855a12c66e0b05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
