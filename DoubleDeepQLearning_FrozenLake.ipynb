{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.__version__\n",
    "# 0.26.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST gym environment\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#env = gym.make('LunarLander')\n",
    "#env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "#env = gym.make('FrozenLake-v1')\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\")\n",
    "\n",
    "observation = env.reset(seed=42)\n",
    "action_size = env.action_space.n\n",
    "print('Number of actions:', action_size)\n",
    "done = False\n",
    "n_episodes = 1\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        action = np.random.randint(0, action_size)\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        print('Action:', action)\n",
    "        print('Observation:', observation)\n",
    "        print('Reward:', reward)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "    done = False\n",
    "    env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer Class\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size):\n",
    "        # Initialize the replay buffer with a given maximum size.\n",
    "        self.buffer_size = buffer_size\n",
    "        # Create an empty buffer list to store experiences.\n",
    "        self.buffer = []\n",
    "        # Initialize the starting position of the buffer.\n",
    "        self.position = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state):\n",
    "        # Add a new experience to the buffer.\n",
    "        experience = (state, action, reward, next_state)\n",
    "        # Add a None element to the buffer list if it's not full yet.\n",
    "        if len(self.buffer) < self.buffer_size:\n",
    "            self.buffer.append(None)\n",
    "        # Add the experience to the buffer at the current position.\n",
    "        self.buffer[self.position] = experience\n",
    "        # Increment the position and wrap around if it goes beyond the buffer size.\n",
    "        self.position = (self.position + 1) % self.buffer_size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        # Sample a random batch of experiences from the buffer.\n",
    "        batch = random.sample(self.buffer, min(batch_size, len(self.buffer)))\n",
    "        # Unzip the batch of experiences into separate lists of states, actions, rewards, and next states.\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        # Convert to a PyTorch tensor with various data type.\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.int64).reshape(-1,1),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the current size of the buffer.\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network and Agent Classes\n",
    "\n",
    "# Define a neural network class that inherits from the PyTorch nn.Module class.\n",
    "class LinearDeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(LinearDeepQNetwork, self).__init__()\n",
    "\n",
    "        # Define the neural network layers and activation functions.\n",
    "        self.fc1 = nn.Linear(input_dims[0], 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "\n",
    "        # Define the optimizer and loss function for training the neural network.\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    # Define the forward pass of the neural network.\n",
    "    def forward(self, state):\n",
    "        layer1 = torch.relu(self.fc1(state))\n",
    "        layer2 = torch.relu(self.fc2(layer1))\n",
    "        actions = self.fc3(layer2)\n",
    "\n",
    "        return actions\n",
    "\n",
    "# Define an agent class for training the neural network.\n",
    "class Agent():\n",
    "    def __init__(self, input_dims, n_actions, lr=1e-4, gamma=0.95,\n",
    "                epsilon=1.0, eps_dec=100e-5, eps_min=0.01):\n",
    "        self.lr = lr\n",
    "        self.input_dims = input_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_dec = eps_dec\n",
    "        self.eps_min = eps_min\n",
    "        self.action_space = [i for i in range(self.n_actions)]\n",
    "\n",
    "        # Create instances of the neural networks for the agent.\n",
    "        self.Q_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        self.Target_network = LinearDeepQNetwork(self.lr, self.n_actions, self.input_dims)\n",
    "        \n",
    "    # Update the target network initially to match the online network.\n",
    "    def update_target_network(self):\n",
    "        self.Target_network.load_state_dict(self.Q_network.state_dict())\n",
    "\n",
    "    # Define a function for choosing an action given an observation.\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            # Use the neural network to predict the Q-values for the current state.\n",
    "            state = torch.tensor(observation, dtype=torch.float).to(self.Q_network.device)\n",
    "            actions = self.Q_network.forward(state)\n",
    "            # Choose the action with the highest Q-value.\n",
    "            action = torch.argmax(actions).item()\n",
    "        else:\n",
    "            # Choose a random action with probability epsilon.\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # Define a function for decrementing epsilon over time to decrease exploration.\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    # Define a function for training the neural network with a batch of experiences.\n",
    "    def learn(self, states, actions, rewards, states_):\n",
    "        self.Q_network.optimizer.zero_grad()\n",
    "        # Convert the data to PyTorch tensors and move to the device for training.\n",
    "        states = torch.tensor(states).to(self.Q_network.device)\n",
    "        actions = torch.tensor(actions).to(self.Q_network.device)\n",
    "        rewards = torch.tensor(rewards).to(self.Q_network.device)\n",
    "        states_ = torch.tensor(states_).to(self.Target_network.device)\n",
    "\n",
    "        # Use the online network to predict the Q-values for the current states and select actions.\n",
    "        q_pred = self.Q_network.forward(states).gather(1, actions)\n",
    "        \n",
    "        # Use the target network to predict the Q-values for the next states.\n",
    "        q_next, _ = torch.max(self.Target_network.forward(states_), dim=1)\n",
    "\n",
    "        # Calculate the target Q-values based on the current rewards and expected future rewards.\n",
    "        q_target = rewards + self.gamma * q_next\n",
    "\n",
    "        # Calculate the mean squared error loss between the predicted and target Q-values.\n",
    "        loss = self.Q_network.loss(q_target, q_pred.squeeze()).to(self.Q_network.device)\n",
    "        # Perform backpropagation to update the online network weights.\n",
    "        loss.backward()\n",
    "        self.Q_network.optimizer.step()\n",
    "        # Decrease the epsilon value to decrease exploration over time.\n",
    "        self.decrement_epsilon()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert integer to array\n",
    "def int_to_array(intn):\n",
    "    # Create a numpy array of zeros with 16 elements.\n",
    "    bin_arr = np.zeros(16)\n",
    "    # Set the element of the numpy array at index intn to 1.\n",
    "    bin_arr[intn] = 1\n",
    "    return bin_arr\n",
    "\n",
    "# Print the binary array representation of the integer 15.\n",
    "print(int_to_array(15))\n",
    "\n",
    "# Define a function for managing rewards based on the current and next observations.\n",
    "def reward_managment(obs, obs_):\n",
    "    # If the next observation is in a specific set of states, return a negative reward.\n",
    "    if obs_ in [5, 7, 11, 12]:\n",
    "        return -1\n",
    "    # If the next observation is the same as the current observation, return a negative reward.\n",
    "    elif (int_to_array(obs_) == obs).all():\n",
    "        return -1\n",
    "    # If the next observation is the goal state, return a positive reward.\n",
    "    elif obs_ == 15:\n",
    "        return 10\n",
    "    # Otherwise, return a small negative reward.\n",
    "    else:\n",
    "        return -0.1\n",
    "\n",
    "# plot score and epsilon curves\n",
    "def plot_score_epsilon(x_axis, scores, epsilons):\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot the first curve using the first y-axis.\n",
    "    ax1.plot(x_axis, scores, 'b-', label='score')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('score', color='b')\n",
    "\n",
    "    # Create a second y-axis object and plot the second curve using it.\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x_axis, epsilons, 'r-', label='epsilon')\n",
    "    ax2.set_ylabel('epsilon', color='r')\n",
    "\n",
    "    # Add a legend to the plot.\n",
    "    lines, labels = ax1.get_legend_handles_labels()\n",
    "    lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "    ax2.legend(lines + lines2, labels + labels2, loc='lower left')\n",
    "\n",
    "    # Show the plot.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FrozenLake-v1 environment with slippery mode disabled.\n",
    "env = gym.make('FrozenLake-v1', is_slippery=False)\n",
    "\n",
    "# Define the number of episodes to run and initialize tracking variables.\n",
    "n_episodes = 200\n",
    "target_update_freq = 5\n",
    "x_axis, scores, epsilons = [], [], []\n",
    "\n",
    "# Create an instance of the agent class.\n",
    "agent = Agent(input_dims=(16,), n_actions=env.action_space.n)\n",
    "\n",
    "# Initialize the replay buffer.\n",
    "buffer_size = 32\n",
    "batch_size = 8\n",
    "replay_buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "# Run the main training loop for the specified number of episodes.\n",
    "for i in range(n_episodes):\n",
    "    # Reset the environment and initialize the score, done flag, and observation.\n",
    "    score = 0\n",
    "    done = False\n",
    "    state = int_to_array(env.reset(seed=42)[0])\n",
    "    steps = 0\n",
    "\n",
    "    # Run the episode until the environment returns done.\n",
    "    while not done:\n",
    "        # Choose an action based on the current observation and agent policy.\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        # Take a step in the environment and update the score, done flag, and observation.\n",
    "        state_, reward, done, info, _ = env.step(action)\n",
    "        reward = reward_managment(state, state_)\n",
    "        replay_buffer.add(state, action, reward, int_to_array(state_))\n",
    "        if i>batch_size:\n",
    "            states, actions, rewards, states_ = replay_buffer.sample(batch_size)\n",
    "            agent.learn(states, actions, rewards, states_)\n",
    "        state = int_to_array(state_)\n",
    "        score += reward\n",
    "        steps += 1\n",
    "\n",
    "        # If the episode runs for more than 20 steps, force it to end.\n",
    "        if steps>20:\n",
    "            break\n",
    "    \n",
    "    if i % target_update_freq == 0:\n",
    "        agent.update_target_network()\n",
    "\n",
    "    # Print the episode statistics and track the score and epsilon value over time.\n",
    "    if (i+1) % 5 == 0:\n",
    "        avg_score = np.mean(scores[-5:])\n",
    "        print(f'episode {i+1}, last score {score:.1f}, avg score {avg_score:.1f} epsilon {agent.epsilon:.2f}')\n",
    "        x_axis.append(i+1)\n",
    "        scores.append(score)\n",
    "        epsilons.append(agent.epsilon)\n",
    "\n",
    "# Plot the scores and epsilon values over time.\n",
    "plot_score_epsilon(x_axis, scores, epsilons)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries.\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Set the exploration rate to zero to force the agent to choose actions based on its learned policy.\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "# Create an instance of the FrozenLake-v1 environment with slippery mode disabled and rendering enabled.\n",
    "env = gym.make('FrozenLake-v1', render_mode=\"human\", is_slippery=False)\n",
    "\n",
    "# Define a dictionary to map actions to human-readable names.\n",
    "action_dict = {0: 'left', 1:'down', 2:'right', 3:'left'}\n",
    "\n",
    "# Convert the initial observation returned by the environment to a numpy array.\n",
    "observation = int_to_array(env.reset(seed=42)[0])\n",
    "\n",
    "# Get the number of actions in the environment.\n",
    "action_size = env.action_space.n\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# Run a single episode of the environment using the agent's learned policy.\n",
    "done = False\n",
    "n_episodes = 1\n",
    "for _ in range(n_episodes):\n",
    "    while not done:\n",
    "        # Print the current observation and chosen action.\n",
    "        print('---')\n",
    "        print('Observation:', observation)\n",
    "        action = agent.choose_action(observation)\n",
    "        print('Action:', action_dict[action])\n",
    "        \n",
    "        # Take a step in the environment and render the current state.\n",
    "        observation, reward, done, info, _ = env.step(action)\n",
    "        observation = int_to_array(observation)\n",
    "        env.render()\n",
    "        #time.sleep(0.1)\n",
    "\n",
    "    # Reset the environment and the done flag for the next episode.\n",
    "    done = False\n",
    "    env.reset(seed=42)\n",
    "\n",
    "# Close the environment viewer.\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
